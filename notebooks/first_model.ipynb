{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_config (SEED_VALUE=42, package_list=None):\n",
    "\n",
    "    random.seed(SEED_VALUE)\n",
    "    np.random.seed(SEED_VALUE)\n",
    "    torch.manual_seed(SEED_VALUE)\n",
    "\n",
    "    def is_running_in_kaggle():\n",
    "        return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "    \n",
    "    #GPU check\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA GPU\")\n",
    "\n",
    "        if is_running_in_kaggle():\n",
    "            print(\"Installing required packages...\")\n",
    "            !pip install {package_list}\n",
    "        \n",
    "        DEVICE = torch.device(\"cuda\")\n",
    "        print(\"Devide: \",  DEVICE)\n",
    "        GPU_AVAILABLE = True\n",
    "\n",
    "        torch.cuda.manual_seed_all(SEED_VALUE)\n",
    "        torch.cuda.manual_seed(SEED_VALUE)\n",
    "\n",
    "        torch.backends.cudnn.enabled = True       # Provides highly optimized primitives for DL operations.\n",
    "        torch.backends.cudnn.deterministic = True # Insures deterministic even when above cudnn is enabled.\n",
    "        torch.backends.cudnn.benchmark = False    # Setting to True can cause non-deterministic behavior.\n",
    "\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        print('Using Apple Silicon GPU')\n",
    "\n",
    "        # Set the device to the Apple Silicon GPU Metal Performance Shader (MPS).\n",
    "        DEVICE = torch.device(\"mps\")\n",
    "        print(\"Device: \", DEVICE)\n",
    "        # Environment variable that allows PyTorch to fall back to CPU execution\n",
    "        # when encountering operations that are not currently supported by MPS.\n",
    "        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "        GPU_AVAILABLE = True\n",
    "\n",
    "        torch.mps.manual_seed(SEED_VALUE)\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "        DEVICE = torch.device('cpu')\n",
    "        print(\"Device: \", DEVICE)\n",
    "        GPU_AVAILABLE = False\n",
    "\n",
    "        if is_running_in_kaggle():\n",
    "            print('Installing required packages...')\n",
    "            !pip install {package_list}\n",
    "            print('Note: Change runtime type to GPU for better performance.')\n",
    "\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    return str(DEVICE), GPU_AVAILABLE\n",
    "\n",
    "DEVICE, GPU_AVAILABLE = system_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entorn variables\n",
    "RANDOM_SEED = 42\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "LABEL_WEIGHTS = {\n",
    "    \"Dry_Green_g\": 0.1,\n",
    "    \"Dry_Dead_g\": 0.1,\n",
    "    \"Dry_Clover_g\": 0.1,\n",
    "    \"GDM_g\": 0.2,\n",
    "    \"Dry_Total_g\": 0.5\n",
    "}\n",
    "N_SPLITS = 5 \n",
    "TARGET_NAMES= list(LABEL_WEIGHTS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement competition loss function \n",
    "def globally_weighted_r_squared(y_true: torch.tensor, y_pred: torch.tensor, target_class: list[str], eps: float = 1e-12) -> torch.tensor:\n",
    "    y_true = y_true.view(-1)\n",
    "    y_pred = y_pred.view(-1)\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shapes must match. Got {y_true.shape} vs {y_pred.shape}.\")\n",
    "\n",
    "    weights = torch.tensor([LABEL_WEIGHTS[cls] for cls in target_class], device=y_true.device, dtype=y_true.dtype)\n",
    "\n",
    "    y_bar = torch.sum(weights * y_true)/weights.sum()\n",
    "    ss_res = torch.sum(weights * (y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum(weights * (y_true - y_bar) ** 2) + eps\n",
    "\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    return r_squared\n",
    "\n",
    "globally_weighted_r_squared(torch.tensor([1,2,3,4,5]), torch.tensor([0,0,0,0,0]),[\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\", \"GDM_g\", \"Dry_Total_g\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement robust validation set\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"raw\", \"train.csv\"))\n",
    "train_df[\"id\"] = train_df[\"image_path\"].apply(lambda row: row.replace(\"train/\", \"\").replace(\".jpg\", \"\"))\n",
    "train_df_ids = train_df.groupby(\"id\", as_index=False).agg(\n",
    "    Pre_GSHH_NDVI = (\"Pre_GSHH_NDVI\", \"first\"),\n",
    "    Height_Ave_cm = (\"Height_Ave_cm\", \"first\"),\n",
    ")\n",
    "\n",
    "train_df_ids[\"ndvi_bin\"] = pd.qcut(train_df_ids[\"Pre_GSHH_NDVI\"], q=3, duplicates=\"drop\")\n",
    "train_df_ids[\"h_bin\"] = pd.qcut(train_df_ids[\"Height_Ave_cm\"], q=3, duplicates=\"drop\")\n",
    "\n",
    "train_df_ids[\"stratify_group\"] = train_df_ids[\"ndvi_bin\"].astype(str) + \"_\" + train_df_ids[\"h_bin\"].astype(str)\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state= RANDOM_SEED)\n",
    "\n",
    "train_df_ids[\"fold\"] = -1\n",
    "\n",
    "X = train_df_ids[[\"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]]\n",
    "y = train_df_ids[\"stratify_group\"]\n",
    "groups = train_df_ids[\"id\"]\n",
    "\n",
    "for fold, (_, val_idx) in enumerate(sgkf.split(X, y, groups=groups)):\n",
    "    train_df_ids.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "train_df = train_df.merge(train_df_ids[[\"id\", \"fold\"]], on=\"id\", how=\"left\")\n",
    "assert train_df[\"fold\"].isna().sum() == 0\n",
    "\n",
    "check = train_df_ids.groupby(\"fold\")[[\"Pre_GSHH_NDVI\", \"Height_Ave_cm\"]].describe()\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0 #Choose validation fold\n",
    "\n",
    "train_df_fold = train_df[train_df[\"fold\"] != FOLD].reset_index(drop=True)\n",
    "val_df_fold = train_df[train_df[\"fold\"] == FOLD].reset_index(drop=True)\n",
    "\n",
    "assert set(train_df_fold[\"id\"]).isdisjoint(set(val_df_fold[\"id\"]))\n",
    "\n",
    "def build_targets(df, target_names=TARGET_NAMES):\n",
    "    return (\n",
    "        df.pivot(index=\"id\", columns=\"target_name\", values=\"target\")\n",
    "        .loc[:, target_names]\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "train_targets = build_targets(train_df_fold, TARGET_NAMES)\n",
    "val_targets = build_targets(val_df_fold, TARGET_NAMES)\n",
    "\n",
    "val_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassDataset(Dataset):\n",
    "    def __init__(self, df, image_root, transform=None):\n",
    "        self.df = df\n",
    "        self.image_root = image_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_path = os.path.join(self.image_root, row[\"image_path\"])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = image[:, :, 1] #Take only green chanel \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        y =  torch.tensor(\n",
    "            row[TARGET_NAMES].values,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BiomassDataset(\n",
    "    train_targets, \n",
    "    image_root=os.path.join(DATA_DIR, \"resized\"),\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_df = BiomassDataset(\n",
    "    val_targets,\n",
    "    image_root=os.path.join(DATA_DIR, \"resized\"),\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_df, batch_size=16, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_df, batch_size=16, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_outputs = len(TARGET_NAMES)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=64*128, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=self.n_outputs)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x= self.model(x)\n",
    "        return x\n",
    "\n",
    "def train(DEVICE: torch.device,\n",
    "          model: nn.Module,\n",
    "          optimizer: optim.Optimizer,\n",
    "          train_loader: DataLoader,\n",
    "          epoch_index: int\n",
    "           ) -> Tuple[float, float]:\n",
    "    \n",
    "    model.train() # Set model to training mode\n",
    "    model.to(DEVICE) # Move model to device\n",
    "\n",
    "    step_loss = 0.0\n",
    "    step_r_squared = 0.0\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_target_class = []\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward\n",
    "        outputs = model(images)\n",
    "        loss = nn.MSELoss()(outputs, targets)\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        step_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size \n",
    "\n",
    "        B, T = targets.shape\n",
    "        all_y_true.append(targets.detach().cpu().view(-1))\n",
    "        all_y_pred.append(outputs.detach().cpu().view(-1))\n",
    "\n",
    "        all_target_class.extend(TARGET_NAMES * B)\n",
    "\n",
    "    epoch_loss = step_loss/n_samples\n",
    "    y_true = torch.cat(all_y_true)\n",
    "    y_pred = torch.cat(all_y_pred)\n",
    "    epoch_r2 = globally_weighted_r_squared(y_true, y_pred, all_target_class).item()\n",
    "    print(f\"Epoch {epoch_index}: Train loss={epoch_loss:.4f}, R^2={epoch_r2:.4f}\")\n",
    "    return epoch_loss, epoch_r2\n",
    "\n",
    "def validate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
